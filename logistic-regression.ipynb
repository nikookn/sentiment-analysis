{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Logistic Regression\n",
    "\n",
    "We will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the necessary datasets.\n",
    "- ``twitter_samples``: Check out the documentation for the [``twitter_samples`` dataset](http://www.nltk.org/howto/twitter.html).\n",
    "- ``stopwords``\n",
    "\n",
    "Uncomment the next cell if you have not downloaded these datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The ``twitter_samples`` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n",
    "- If we used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n",
    "- We will select just the five thousand positive tweets and five thousand negative tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the numpy array of positive labels and negative labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming\n",
    "\n",
    "Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and     # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mAn example of a positive tweet: \n",
      "\u001b[34m #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\u001b[0m\n",
      "An example of the processed version of the tweet: \n",
      "\u001b[32m ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('\\033[0mAn example of a positive tweet: \\n\\033[34m', train_x[0])\n",
    "print('\\033[0m\\nAn example of the processed version of the tweet: \\n\\033[32m', process_tweet(train_x[0]))\n",
    "print('\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the frequency dictionary ``freqs``. The key is the tuple ``(word, label)``, such as ``(\"happy\",1)`` or ``(\"happy\",0)``. The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "               frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11340\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the features\n",
    "\n",
    "Given a list of tweets, we extract the features and store them in a matrix. We will extract two features.\n",
    "\n",
    "- The first feature is the number of positive words in a tweet.\n",
    "- The second feature is the number of negative words in a tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3))\n",
    "    x[0,0] = 1 #bias term is set to 1\n",
    "    \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1), 0)\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0), 0)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "\n",
    "Note that the $\\theta$ values are \"weights\".\n",
    "\n",
    "Logistic regression\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "\n",
    "We will refer to 'z' as the 'logits'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    h = 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))$$\n",
    "\n",
    "- $m$ is the number of training examples\n",
    "- $y^{(i)}$ is the actual label of the i-th training example.\n",
    "- $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "### Update the weights\n",
    "\n",
    "To update our weight vector $\\theta$, we will apply gradient descent to iteratively improve our model's predictions.\n",
    "\n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j$$\n",
    "- 'i' is the index across all 'm' training examples.\n",
    "- 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "- The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "- $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "\n",
    "The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "- $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "- $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "- $\\mathbf{z}$: has dimensions (m, 1)\n",
    "\n",
    "The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "\n",
    "The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "\n",
    "The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations we want to train our model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: the final weight vector\n",
    "    '''\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    J_list = [] # cost over iteration\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        # calculate the cost function\n",
    "        J = - 1.0 / m * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta - alpha / m * np.dot(x.T, h - y)\n",
    "        \n",
    "        # add the cost to the list\n",
    "        J_list.append(float(J))\n",
    "        \n",
    "    return float(J), theta, np.array(J_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train the model, let's stack the features for all training examples into a matrix X and call ``gradient_descent``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24216477.\n",
      "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Alpha: learning rate\n",
    "alpha = 1e-9\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 1500\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta, J_history = gradient_descent(X, Y, np.zeros((3, 1)), alpha, n_iter)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(J_history)\n",
    "plt.xticks(np.arange(0, n_iter+100, step=300))\n",
    "plt.yticks(np.arange(0, 1.0, step=0.1))\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the logistic regression\n",
    "\n",
    "It is time to test the logistic regression function on some new input \n",
    "that the model has not seen during the training.\n",
    "\n",
    "First, we write a function, ``logistic_regression_predict``, that predicts whether \n",
    "a tweet is positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_predict(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of trained weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we write a function, ``logistic_regression_evaluate``, that given \n",
    "the test data and the weights of our trained model, it calculates \n",
    "the accuracy of our logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_evaluate(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: trained weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = logistic_regression_predict(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred >= 0.5:\n",
    "            y_hat.append(1.0) # append 1.0 to the list\n",
    "        else:\n",
    "            y_hat.append(0.0) # append 0.0 to the list\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    accuracy = np.sum(np.asarray(y_hat).reshape((-1, 1)) == test_y) / test_y.shape[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "accuracy = logistic_regression_evaluate(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "In this part we will see some tweets that our model misclassified. Let's see what kind of tweets does our model misclassify?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "\n",
      "\u001b[31m1\t0.49996897\t\u001b[32mb'truli later move know queen bee upward bound movingonup'\n",
      "\u001b[31m1\t0.48650628\t\u001b[32mb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "\u001b[31m1\t0.48370676\t\u001b[32mb\"i'm play brain dot braindot\"\n",
      "\u001b[31m1\t0.48370676\t\u001b[32mb\"i'm play brain dot braindot\"\n",
      "\u001b[31m1\t0.48370676\t\u001b[32mb\"i'm play brain dot braindot\"\n",
      "\u001b[31m1\t0.49578773\t\u001b[32mb'park get sunlight'\n",
      "\u001b[31m1\t0.48199817\t\u001b[32mb'uff itna miss karhi thi ap :p'\n",
      "\u001b[31m0\t0.50020361\t\u001b[32mb'u prob fun david'\n",
      "\u001b[31m0\t0.50039294\t\u001b[32mb'pat jay'\n",
      "\u001b[31m0\t0.50000002\t\u001b[32mb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis\n",
    "print('Label Predicted Tweet\\n')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = logistic_regression_predict(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat >= 0.5)) > 0:\n",
    "        # print('\\033[0mTHE TWEET IS: \\033[34m', x)\n",
    "        # print('\\033[0mTHE PROCESSED TWEET IS: \\033[32m', process_tweet(x))\n",
    "        print('\\033[31m%d\\t%0.8f\\t\\033[32m%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Deep Neural Networks\n",
    "\n",
    "We will explore sentiment analysis on tweets using deep neural networks. Given a tweet, we will decide if it has a positive sentiment or a negative one.\n",
    "\n",
    "Given an example like: \"This movie was almost good.\", Logistic regression and Naive Bayes models will predict a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first download the necessary datasets.\n",
    "- ``twitter_samples``: Check out the documentation for the [``twitter_samples`` dataset](http://www.nltk.org/howto/twitter.html).\n",
    "- ``stopwords``\n",
    "\n",
    "Uncomment the next cell if you have not downloaded these datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('twitter_samples')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import random as rnd\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds to make the results easier to replicate\n",
    "manual_seed = 123\n",
    "rnd.seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "The ``twitter_samples`` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.\n",
    "- If we used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.\n",
    "- We will select just the five thousand positive tweets and five thousand negative tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the numpy array of positive labels and negative labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000,)\n",
      "test_y.shape = (2000,)\n"
     ]
    }
   ],
   "source": [
    "# combine positive and negative labels (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))\n",
    "\n",
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming\n",
    "\n",
    "Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and     # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)        # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mAn example of a positive tweet: \n",
      "\u001b[34m #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\u001b[0m\n",
      "An example of the processed version of the tweet: \n",
      "\u001b[32m ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('\\033[0mAn example of a positive tweet: \\n\\033[34m', train_x[0])\n",
    "print('\\033[0m\\nAn example of the processed version of the tweet: \\n\\033[32m', process_tweet(train_x[0]))\n",
    "print('\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function ``process_tweet`` keeps key words, removes the hash # symbol, and ignores usernames (words that begin with '@').  It also returns a list of the words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "\n",
    "Now, let's build the vocabulary based on the training data. \n",
    "We will map each word in each tweet to an integer (an \"index\"). \n",
    "The vocabulary will also include some special tokens:\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    }
   ],
   "source": [
    "# Include special tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary `Vocab` will look like this:\n",
    "```python\n",
    "{'__PAD__': 0,\n",
    " '__</e>__': 1,\n",
    " '__UNK__': 2,\n",
    " 'followfriday': 3,\n",
    " 'top': 4,\n",
    " 'engag': 5,\n",
    " ...\n",
    "```\n",
    "\n",
    "- Each unique word has a unique integer associated with it.\n",
    "- The total number of words in Vocab: 9088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a tweet to a tensor\n",
    "\n",
    "Now, let's write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet). \n",
    "- Note, the returned data type will be a **regular Python ``list()``**, not a numpy array.\n",
    "- For words in the tweet that are not in the vocabulary, we'll set them to the unique ID for the token ``__UNK__``.\n",
    "\n",
    "#### Example\n",
    "Input a tweet:\n",
    "```python\n",
    "'@happypuppy, is Maria happy?'\n",
    "```\n",
    "\n",
    "The ``tweet_to_tensor`` will first conver the tweet into a list of tokens (including only relevant words)\n",
    "```python\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then, it will convert each word into its unique integer\n",
    "```python\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the ``__UNK__`` token, because it is considered \"unknown.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        tweet:      A string containing a tweet\n",
    "        vocab_dict: The words dictionary\n",
    "        unk_token:  The special string for unknown tokens\n",
    "        verbose:    Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l:   A python list with unique integer IDs representing \n",
    "                    the processed tweet\n",
    "    \"\"\"\n",
    "    # Process the tweet into a list of words (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        # Get the unique integer ID of the word, or \n",
    "        # the unique ID for __UNK__ if the word doesn't exist in the vocab dictionary.\n",
    "        word_ID = vocab_dict.get(word, unk_ID)\n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is:\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is:\\n\", test_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(test_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a batch generator\n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general, we use batches when training our datasets. \n",
    "- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n",
    "- We will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \n",
    "\n",
    "Once we created the generator, we could include it in a for loop like this:\n",
    "```python\n",
    "for batch_inputs, batch_targets, batch_example_weights in data_generator:\n",
    "    ...\n",
    "```\n",
    "\n",
    "We can also get a single batch like this:\n",
    "```python\n",
    "batch_inputs, batch_targets, batch_example_weights = next(data_generator)\n",
    "```\n",
    "\n",
    "The generator returns the next batch each time it's called. \n",
    "- This generator returns the data in a format (tensors) that we could directly use in our model.\n",
    "- It returns a triple: the inputs, targets, and loss weights:\n",
    "  - Inputs is a tensor that contains the batch of tweets we put into the model.\n",
    "  - Targets is the corresponding batch of labels that we train to generate.\n",
    "  - Loss weights here are just 1s with same shape as targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        data_pos:   Set of posstive examples\n",
    "        data_neg:   Set of negative examples\n",
    "        batch_size: number of samples per batch. Must be even\n",
    "        loop:       whether to loop over data or not (True or False)\n",
    "        vocab_dict: The words dictionary\n",
    "        shuffle:    Shuffle the data order (True or False)\n",
    "    Yield:\n",
    "        inputs:          Subset of positive and negative examples\n",
    "        targets:         The corresponding labels for the subset\n",
    "        example_weights: An array specifying the importance of each example\n",
    "    \"\"\"\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive and negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # pos_index and neg_index to walk through the data_pos and data_neg arrays\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    # Loop indefinitely\n",
    "    while not stop:\n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        for i in range(n_to_take):\n",
    "            # If the positive index goes past the positive dataset lenght\n",
    "            if pos_index >= len_data_pos: \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "        for i in range(n_to_take):\n",
    "            # If the negative index goes past the negative dataset length\n",
    "            if neg_index >= len_data_neg:\n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            # Increment neg_index by one\n",
    "            neg_index = neg_index + 1\n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # # Update the start index for positive and negative data \n",
    "        # # so that it's n_to_take positions after the current pos_index and neg_index\n",
    "        # pos_index += n_to_take\n",
    "        # neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (we will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch])\n",
    "        \n",
    "        # Initialize the tensor_pad_l, which will store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "            # Get the number of positions to pad for this tensor\n",
    "            n_pad = max_len - len(tensor)\n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0] * n_pad\n",
    "            # Concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            # Append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # The list of targets for the positive examples (a list of ones)\n",
    "        target_pos = [1] * n_to_take\n",
    "        # The list of targets for the negative examples (a list of zeros)\n",
    "        target_neg = [0] * n_to_take\n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the list of padded tensors and the target list to numpy arrays\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "        targets = np.array(target_l)\n",
    "        # Example weights: Treat all examples equally important.\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our ``data_generator`` to create a data generator for the training data, and another data generator for the test data. We will create a third data generator that does not loop, for testing the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(manual_seed) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle=False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle=False):\n",
    "    return data_generator(test_pos, test_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle=False):\n",
    "    return data_generator(test_pos, test_neg, batch_size, False, Vocab, shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[3327   94   57 5660   75    0    0    0]\n",
      " [ 417 1397   22   95    9    0    0    0]\n",
      " [ 774  172 6558  377  621 1571  307 3761]\n",
      " [ 710  236  293 2193 2306 8093  486 3761]]\n",
      "Targets:\n",
      " [1 1 0 0]\n",
      "Example Weights:\n",
      " [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# This will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs:\\n {inputs}')\n",
    "print(f'Targets:\\n {targets}')\n",
    "print(f'Example Weights:\\n {example_weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our train/val generators, we can just call them and they will return tensors which correspond to our tweets in the first column and their corresponding labels in the second column. Now we can go ahead and start building our neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we will implement a classifier using neural networks. Here is the model architecture we will be implementing. \n",
    "\n",
    "<img src = \"images/nn.jpg\" style=\"width:400px;height:250px;\"/>\n",
    "\n",
    "For the model implementation, we will use the Trax layers library `tl`. \n",
    "Note that the second character of `tl` is the lowercase of letter `L`, not the number 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        vocab_size:    size of the vocabulary\n",
    "        embedding_dim: number of features in embedding layer\n",
    "        output_dim:    number of output in classifier\n",
    "        mode:          mode of classifier\n",
    "    Output:\n",
    "        model: the model of type trax.layers.combinators.Serial\n",
    "    \"\"\"\n",
    "    # Embedding layer\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size,    # Size of the vocabulary\n",
    "        d_feature=embedding_dim)  # Embedding dimension\n",
    "    # Mean layer, to create an \"average\" over word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    # Dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units=output_dim)\n",
    "    # Log softmax layer (no parameters needed)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    # Use tl.Serial to combine all layers and create the classifier\n",
    "    model = tl.Serial(\n",
    "      embed_layer,        # embedding layer\n",
    "      mean_layer,         # mean layer\n",
    "      dense_output_layer, # dense output layer \n",
    "      log_softmax_layer   # log softmax layer\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9088_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = classifier()\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "To train a model on a task, Trax defines an abstraction [``trax.supervised.training.TrainTask``](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly, to evaluate a model, Trax defines an abstraction [``trax.supervised.training.EvalTask``](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the [``trax.supervised.training.Loop``](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "Using ``Loop`` will save us a lot of code compared to always writing the training loop by hand. More importantly, we are less likely to have a bug in that code that would ruin our training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./venv/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "rnd.seed(manual_seed)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines a model trained using [``tl.CrossEntropyLoss``](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss) optimized with the [``trax.optimizers.Adam``](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam) optimizer, all the while tracking the accuracy using [``tl.Accuracy``](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy) metric. We also track ``tl.CrossEntropyLoss`` on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        classifier: the model\n",
    "        train_task: Training task\n",
    "        eval_task:  Evaluation task\n",
    "        n_steps:    the evaluation steps\n",
    "        output_dir: directory to save files\n",
    "    Output:\n",
    "        training_loop: trax trainer\n",
    "    \"\"\"\n",
    "    training_loop = training.Loop(classifier,              # The learning model\n",
    "                                  train_task,              # The training task\n",
    "                                  eval_tasks=[eval_task],  # The evaluation task\n",
    "                                  output_dir=output_dir,   # The output directory\n",
    "                                  random_seed=manual_seed) # random seed\n",
    "    \n",
    "    training_loop.run(n_steps = n_steps)\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make an output directory and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step    110: Ran 10 train steps in 3.70 secs\n",
      "Step    110: train CrossEntropyLoss |  0.01230888\n",
      "Step    110: eval  CrossEntropyLoss |  0.01557323\n",
      "Step    110: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    120: Ran 10 train steps in 3.15 secs\n",
      "Step    120: train CrossEntropyLoss |  0.02942829\n",
      "Step    120: eval  CrossEntropyLoss |  0.17076454\n",
      "Step    120: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    130: Ran 10 train steps in 1.68 secs\n",
      "Step    130: train CrossEntropyLoss |  0.03062473\n",
      "Step    130: eval  CrossEntropyLoss |  0.06471900\n",
      "Step    130: eval          Accuracy |  0.93750000\n",
      "\n",
      "Step    140: Ran 10 train steps in 1.31 secs\n",
      "Step    140: train CrossEntropyLoss |  0.02525533\n",
      "Step    140: eval  CrossEntropyLoss |  0.01430054\n",
      "Step    140: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    150: Ran 10 train steps in 0.68 secs\n",
      "Step    150: train CrossEntropyLoss |  0.04391470\n",
      "Step    150: eval  CrossEntropyLoss |  0.01726753\n",
      "Step    150: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    160: Ran 10 train steps in 2.22 secs\n",
      "Step    160: train CrossEntropyLoss |  0.01167804\n",
      "Step    160: eval  CrossEntropyLoss |  0.01989873\n",
      "Step    160: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    170: Ran 10 train steps in 1.76 secs\n",
      "Step    170: train CrossEntropyLoss |  0.02452637\n",
      "Step    170: eval  CrossEntropyLoss |  0.01157440\n",
      "Step    170: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    180: Ran 10 train steps in 0.79 secs\n",
      "Step    180: train CrossEntropyLoss |  0.01328370\n",
      "Step    180: eval  CrossEntropyLoss |  0.00316454\n",
      "Step    180: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    190: Ran 10 train steps in 0.66 secs\n",
      "Step    190: train CrossEntropyLoss |  0.01583867\n",
      "Step    190: eval  CrossEntropyLoss |  0.00679881\n",
      "Step    190: eval          Accuracy |  1.00000000\n",
      "\n",
      "Step    200: Ran 10 train steps in 1.34 secs\n",
      "Step    200: train CrossEntropyLoss |  0.00879420\n",
      "Step    200: eval  CrossEntropyLoss |  0.01156633\n",
      "Step    200: eval          Accuracy |  1.00000000\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./model/\"\n",
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained a model, we can access it as ``training_loop.model`` object. We will actually use ``training_loop.eval_model``. We sometimes use a different model for evaluation, e.g., one without dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Computing the accuracy on a batch\n",
    "\n",
    "We will now write a function that evaluates our model on the validation set and returns the accuracy. \n",
    "- ``preds`` contains the predictions.\n",
    "    - Its dimensions are ``(batch_size, output_dim)``. ``output_dim`` is two in this case. Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).\n",
    "    - If the probability in column 1 is greater than the probability in column 0, then interpret this as the model's prediction that the example has label 1 (positive sentiment).  \n",
    "    - Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model's prediction is 0 (negative sentiment).\n",
    "- ``y`` contains the actual labels.\n",
    "- ``y_weights`` contains the weights to give to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds:     a tensor of shape (dim_batch, output_dim) \n",
    "        y:         a tensor of shape (dim_batch, output_dim) with the true labels\n",
    "        y_weights: a (np.ndarray) with the a weight for each example\n",
    "    Output: \n",
    "        accuracy:             a float between 0-1 \n",
    "        weighted_num_correct: Sum of the weighted correct predictions (np.float32)\n",
    "        sum_weights:          Sum of the weights (np.float32)\n",
    "    \"\"\"\n",
    "    # An array of np.int32: `1` if the probability of positive sentiment\n",
    "    # is greater than the probability of negative sentiment, else `0`\n",
    "    is_pos_int = (preds[:, 0] < preds[:, 1]).astype(np.int32)\n",
    "    # The array of correct predictions of np.float32\n",
    "    correct_float = (is_pos_int == y).astype(np.float32)\n",
    "    # Multiply each prediction with its corresponding weight\n",
    "    weighted_correct_float = np.multiply(correct_float, y_weights)\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32)\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    "    # The sum of the weights\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    # Divide the number of weighted correct predictions by the sum of the weights\n",
    "    accuracy = weighted_num_correct / sum_weights\n",
    "\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "Now we will test our model's prediction accuracy on validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(generator, model):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model:     a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    \"\"\"\n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    for batch in generator: \n",
    "        # Retrieve the inputs, the targets (actual labels) and the example weight\n",
    "        inputs, targets, example_weight = batch\n",
    "        # predictions using the inputs\n",
    "        pred = model(inputs)\n",
    "        # Accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
    "        # Update the total number of predictions and correct predictions\n",
    "        total_num_correct += batch_num_correct\n",
    "        total_num_pred += batch_num_pred\n",
    "        \n",
    "    # Accuracy over all examples\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model on the validation set is 0.9940\n"
     ]
    }
   ],
   "source": [
    "# Testing the accuracy of the model\n",
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of the model on the validation set is {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a tweet\n",
    "\n",
    "Finally we will test with new input tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]\n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    if preds == 1:\n",
    "        sentiment = \"positive\"\n",
    "    else:\n",
    "        sentiment = \"negative\"\n",
    "\n",
    "    return preds, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the sentence \n",
      "***\n",
      "\"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
      "***\n",
      "is positive.\n",
      "\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I hated my day, it was the worst, I'm so sad.\"\n",
      "***\n",
      "is negative.\n"
     ]
    }
   ],
   "source": [
    "# Try a positive sentence\n",
    "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model works well even for complex sentences.\n",
    "\n",
    "Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression.\n",
    "- It also allows us to better use pre-trained embeddings for classification and tends to generalize better.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
